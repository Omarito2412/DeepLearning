#### MNIST results using Neural Network with different parameters
In this document I will gather the results I achieved using a simple neural network on the MNIST dataset.

The first result is where I took the network a little bit too far using a hidden layer of 800 neurons and training it for 150 epochs, it performed a not bad accuracy but still very low for the huge hidden layer.


50 Neurons, No regularization, 200 epochs, 0.3 learning rate, 30% Cross-validation:

##### Sigmoid activation:
![Sigmoid Activation]
()
##### Tanh activation:
![Tanh Activation]
()
##### Leaky ReLU:
![ReLU Activation]
()

