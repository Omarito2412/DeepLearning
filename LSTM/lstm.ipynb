{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras LSTM on the IMDB sentiment dataset\n",
    "### In this notebook you'll find a simple LSTM implementation with details on the Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first nb_words most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words = 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Maximum length of a sequence (Timesteps)  \n",
    " Maximum length in X_train is 2494, it must be cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of each word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of examples to train on simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(nb_words=nb_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now a matrix, each vector is a sequence of integers.  \n",
    "Each integer is a word index in the index table  \n",
    "You can test that out by trying index[\"movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad each sequence, since a neural network can't deal with  \n",
    "variable length input, we need to normalize to a max length  \n",
    "it can be a hyperparameter anyway  \n",
    "Short sequences are padded with zeros  \n",
    "Convolution with mode = \"Same\" has the same effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(X_train, maxlen=sequence_length)\n",
    "test_padded = pad_sequences(X_test, maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sequential model is a type of model which we stack  \n",
    " layers on top of each other sequentially.  \n",
    " Example: Convolution => ReluLayer => Dropout => Pool => Dense => Softmax  \n",
    " Docs: https://keras.io/models/sequential/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an embedding layer, so each integer in sequence is  \n",
    "mapped to a word vector  \n",
    "Example: 17 => [0, 0.1, 0.4, -1.2, 0.2]  \n",
    "In fact this is only a mapping of a word to a learnable word vector  \n",
    "In this example we mapped the word \"Movie\" to that vector  \n",
    "Docs: https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Embedding(nb_words, vector_length, input_length=sequence_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a layer of our beloved LSTM with some dropout  \n",
    "for regularization  \n",
    "Docs: https://keras.io/layers/recurrent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(128, dropout_W=0.4, dropout_U=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a dense layer with sigmoid activation  \n",
    "This is to map our output to a probability  \n",
    "Docs: https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model, calculate loss and choose optimizer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adagrad',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_padded, Y_train, batch_size=batch_size, nb_epoch=epochs)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_padded, Y_test, batch_size=batch_size)\n",
    "print \"The calculated loss is: \" + str(loss)\n",
    "print \"The calculated accuracy is: \" + str(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
